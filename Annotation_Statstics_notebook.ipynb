{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Jacob's Average Score\": 8.2,\n",
       " \"Tim's Average Score\": 7.2,\n",
       " 'Overall Average Score': 7.7,\n",
       " 'Pearson Correlation': 0.8781179573797714,\n",
       " 'Spearman Correlation': 0.8088710220015727,\n",
       " 'Kendall Tau Correlation': 0.7048020519376482,\n",
       " \"Cohen's Kappa\": 0.1361256544502616,\n",
       " 'Mean Score Difference': 1.1333333333333333,\n",
       " 'Exact Score Matches': 4,\n",
       " 'Close Matches (±1)': 11}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Define the function to analyze annotation scores\n",
    "def analyze_annotation_scores(file_path):\n",
    "    \"\"\"\n",
    "    Reads the annotation CSV file and computes key metrics:\n",
    "    - Average scores for both annotators\n",
    "    - Pearson, Spearman, and Kendall correlations\n",
    "    - Cohen’s Kappa for inter-annotator agreement\n",
    "    - Exact and close matches (±1 difference)\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    annotations_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract annotator scores\n",
    "    jacob_scores = annotations_df[\"Jacob's Score\"]\n",
    "    tim_scores = annotations_df[\"Tim's Score\"]\n",
    "\n",
    "    # Compute basic statistics\n",
    "    avg_jacob_score = np.mean(jacob_scores)\n",
    "    avg_tim_score = np.mean(tim_scores)\n",
    "    overall_avg_score = np.mean([jacob_scores, tim_scores])\n",
    "\n",
    "    # Compute Pearson, Spearman, and Kendall correlations\n",
    "    pearson_corr, _ = pearsonr(jacob_scores, tim_scores)\n",
    "    spearman_corr, _ = spearmanr(jacob_scores, tim_scores)\n",
    "    kendall_corr, _ = kendalltau(jacob_scores, tim_scores)\n",
    "\n",
    "    # Compute Cohen's Kappa for inter-annotator agreement\n",
    "    cohen_kappa = cohen_kappa_score(jacob_scores, tim_scores)\n",
    "\n",
    "    # Compute absolute differences to analyze agreement\n",
    "    score_differences = np.abs(jacob_scores - tim_scores)\n",
    "    mean_difference = np.mean(score_differences)\n",
    "    num_exact_matches = np.sum(score_differences == 0)\n",
    "    num_close_matches = np.sum(score_differences <= 1)\n",
    "\n",
    "    # Return computed metrics\n",
    "    return {\n",
    "        \"Jacob's Average Score\": avg_jacob_score,\n",
    "        \"Tim's Average Score\": avg_tim_score,\n",
    "        \"Overall Average Score\": overall_avg_score,\n",
    "        \"Pearson Correlation\": pearson_corr,\n",
    "        \"Spearman Correlation\": spearman_corr,\n",
    "        \"Kendall Tau Correlation\": kendall_corr,\n",
    "        \"Cohen's Kappa\": cohen_kappa,\n",
    "        \"Mean Score Difference\": mean_difference,\n",
    "        \"Exact Score Matches\": num_exact_matches,\n",
    "        \"Close Matches (±1)\": num_close_matches,\n",
    "    }\n",
    "\n",
    "# Define the file path and analyze the annotation scores\n",
    "file_path = \"/Users/jacob/Desktop/MDS/COLX_565/COLX_565_Project_Jacob-Tim/milestone2/Detoxification Annotations.csv\"\n",
    "annotation_metrics = analyze_annotation_scores(file_path)\n",
    "\n",
    "# Display the computed metrics\n",
    "annotation_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

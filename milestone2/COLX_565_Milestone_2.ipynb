{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "hQ0kEMJTRhhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount google drive if on colab:"
      ],
      "metadata": {
        "id": "KxEiErxUSOW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugWLlC_3SJPJ",
        "outputId": "78501705-e1ff-4d9c-a245-8ff6429b3efa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download modules:"
      ],
      "metadata": {
        "id": "_goK3_I8TCSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_huggingface langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4akLSATS4U0",
        "outputId": "0bfcdbc7-1e41-459e-a1eb-0789fbd1c039"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain_huggingface in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (3.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.48.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ],
      "metadata": {
        "id": "o6Gz8R8NR7vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "from langdetect import detect, DetectorFactory\n",
        "import langdetect.lang_detect_exception\n",
        "\n",
        "from transformers import AutoTokenizer, MT5ForConditionalGeneration\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "__sGQEDqR9gn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B-o-Mj_ImjJ2",
        "outputId": "24b91dca-b5aa-460f-eb10-8eaa51cd9aa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data below in the pipeline implementation."
      ],
      "metadata": {
        "id": "DtlgqDVJZR_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models:"
      ],
      "metadata": {
        "id": "fsGnzPH0Sfp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the UBC Toucan model for translation:"
      ],
      "metadata": {
        "id": "GzL4DzwKT9Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To translate using Toucan models, we use the target language ISO-3 code as preix."
      ],
      "metadata": {
        "id": "B2GdX5VWVuOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lang_names={\n",
        "    \"aar\": \"Afar\",\n",
        "    \"ach\": \"Acholi\",\n",
        "    \"afr\": \"Afrikaans\",\n",
        "    \"aka\": \"Akan\",\n",
        "    \"amh\": \"Amharic\",\n",
        "    \"bam\": \"Bambara\",\n",
        "    \"bas\": \"Basaa\",\n",
        "    \"bem\": \"Bemba\",\n",
        "    \"btg\": \"Bete Gagnoa\",\n",
        "    \"eng\": \"English\",\n",
        "    \"ewe\": \"Ewe\",\n",
        "    \"fon\": \"Fon\",\n",
        "    \"fra\": \"French\",\n",
        "    \"hau\": \"Hausa\",\n",
        "    \"ibo\": \"Igbo\",\n",
        "    \"kbp\": \"Kabiye\",\n",
        "    \"lgg\": \"Lugbara\",\n",
        "    \"lug\": \"Luganda\",\n",
        "    \"mlg\": \"Malagasy\",\n",
        "    \"nyn\": \"Nyakore\",\n",
        "    \"orm\": \"Oromo\",\n",
        "    \"som\": \"Somali\",\n",
        "    \"sot\": \"Sesotho\",\n",
        "    \"swa\": \"Swahili\",\n",
        "    \"tir\": \"Tigrinya\",\n",
        "    \"yor\": \"Yoruba\",\n",
        "    \"teo\": \"Ateso\",\n",
        "    \"gez\": \"Geez\",\n",
        "    \"wal\": \"Wolaytta\",\n",
        "    \"fan\": \"Fang\",\n",
        "    \"kau\": \"Kanuri\",\n",
        "    \"kin\": \"Kinyawanda\",\n",
        "    \"kon\": \"Kongo\",\n",
        "    \"lin\": \"Lingala\",\n",
        "    \"nya\": \"Chichewa\",\n",
        "    \"pcm\": \"Nigerian Pidgin\",\n",
        "    \"ssw\": \"Siswati\",\n",
        "    \"tsn\": \"Setswana\",\n",
        "    \"tso\": \"Tsonga\",\n",
        "    \"twi\": \"Twi\",\n",
        "    \"wol\": \"Wolof\",\n",
        "    \"xho\": \"Xhosa\",\n",
        "    \"zul\": \"Zulu\",\n",
        "    \"nnb\": \"Nande\",\n",
        "    \"swc\": \"Swahili Congo\",\n",
        "    \"ara\": \"Arabic\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "b1mUM3XqVno4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#translation_model = HuggingFacePipeline.from_model_id(\n",
        "#    model_id=\"UBC-NLP/toucan-base\",\n",
        "#    task=\"translation\",\n",
        "#    pipeline_kwargs={\n",
        "#        \"num_beams\" : 5,\n",
        "#        \"do_sample\": True,\n",
        "#        \"temperature\": 0.6,\n",
        "#        \"top_p\": 0.9\n",
        "#    }\n",
        "#)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7YwADnwT85M",
        "outputId": "be1a56f3-c369-4f5f-bf98-11ce72713eeb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our Toucan model for African language translation\n",
        "toucan_tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/toucan-base\")\n",
        "toucan_model = MT5ForConditionalGeneration.from_pretrained(\"UBC-NLP/toucan-base\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "toucan_model.eval()\n",
        "\n",
        "# Sanity check that the translation model works: Translate an example from English to Zulu\n",
        "text=\"zul: Clear all items from the recent documents list\"\n",
        "input_ids = toucan_tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(\"cuda:0\")\n",
        "with torch.no_grad():\n",
        "    generated_ids = toucan_model.generate(**input_ids, num_beams=5, max_new_tokens=len(text), do_sample=True, temperature=0.6, top_p=0.9)\n",
        "print(toucan_tokenizer.batch_decode(generated_ids, skip_special_tokens=True,  skip_prompt=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdf3rwGsmBle",
        "outputId": "0b97e9cf-ed27-4650-cf66-d3dd423f350a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vala zonke izinto kusuka kwihlu lamadokhumende elidlule\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the rest of the pipeline, we use the IBM Granite model:"
      ],
      "metadata": {
        "id": "ivnIVKifTwy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "granite_model = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"ibm-granite/granite-3.2-2b-instruct\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\n",
        "        \"return_full_text\": False,\n",
        "        \"do_sample\": False,\n",
        "        \"max_new_tokens\": 300\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "rWgP6OdUSj4Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "4827227cb4274a61829bb6ba13a199ad",
            "5dce9d00434f428d83bf5515a7478816",
            "6e70f9aad9b24139ad192297d65b4c90",
            "11cf8e490fdb407e87459e6370f3ab2d",
            "30a642692b9d4aa0b423cc558d1074b5",
            "4dafcab5097342828681c06845ca2a75",
            "2757d53c0ed2484696b9f6b35565903d",
            "5ce6256c1a61406cb96bd12ab16ef8a5",
            "b269f07013104e3483f9dc970d5c7378",
            "d05321c1f2a34b789cc89711da65023f",
            "9294415bf5ab4595946ae233981b35ef"
          ]
        },
        "outputId": "12217f3e-45a8-414d-aabb-df96c8422b09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4827227cb4274a61829bb6ba13a199ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Detection & Translation"
      ],
      "metadata": {
        "id": "VgqCzciLWmEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DetectorFactory.seed = 0\n",
        "\n",
        "langdetect_to_toucan = {\n",
        "    \"en\": \"eng\", \"fr\": \"fra\", \"sw\": \"swa\", \"yo\": \"yor\", \"ha\": \"hau\",\n",
        "    \"ig\": \"ibo\", \"ar\": \"ara\", \"xh\": \"xho\", \"zu\": \"zul\", \"am\": \"amh\",\n",
        "    \"so\": \"som\", \"st\": \"sot\", \"rw\": \"kin\", \"lg\": \"lug\", \"ln\": \"lin\",\n",
        "    \"sn\": \"tsn\", \"ss\": \"ssw\", \"ny\": \"nya\", \"mg\": \"mlg\", \"om\": \"orm\",\n",
        "    \"ti\": \"tir\", \"nso\": \"tsn\", \"tn\": \"tsn\", \"ts\": \"tso\", \"tw\": \"twi\",\n",
        "    \"wo\": \"wol\", \"kg\": \"kon\", \"ee\": \"ewe\", \"ff\": \"fan\", \"pcm\": \"pcm\",\n",
        "    \"arq\": \"ara\", \"ffm\": \"fan\", \"kab\": \"kab\", \"bm\": \"bam\", \"nyn\": \"nyn\"\n",
        "}\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        detected_lang = detect(text)\n",
        "        return langdetect_to_toucan.get(detected_lang, \"unknown\")\n",
        "    except langdetect.lang_detect_exception.LangDetectException:\n",
        "        return \"unknown\"\n",
        "\n",
        "def detect_and_append_language(df, text_column=\"sentence\"):\n",
        "    df[\"detected_language\"] = df[text_column].apply(detect_language)\n",
        "    return df"
      ],
      "metadata": {
        "id": "zxHHdykKWoJH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language detection chain:"
      ],
      "metadata": {
        "id": "qJ0ypHHtcq_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_language_chain(text):\n",
        "    detected_lang = detect_language(text)\n",
        "    return {\"sentence\": text, \"detected_language\": detected_lang}"
      ],
      "metadata": {
        "id": "BLycCuQJcsoO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translation Framework:"
      ],
      "metadata": {
        "id": "qrk4ig0vZeXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#translation_format_instructions = \"\"\"\n",
        "#Return only a valid JSON object with the following keys:\n",
        "#{\n",
        "#  \"translated_text\": \"The translated English text of the given input.\"\n",
        "#}\n",
        "#\"\"\"\n",
        "#\n",
        "#translation_prompt = ChatPromptTemplate.from_template(\n",
        "#    \"\"\"\n",
        "#    You are an expert in multilingual translation.\n",
        "#    Translate the following sentence into English using the given source language.\n",
        "#\n",
        "#    Source Language: \"{source_language}\"\n",
        "#    Sentence: \"{sentence}\"\n",
        "#\n",
        "#    {format_instructions}\n",
        "#    \"\"\"\n",
        "#).partial(format_instructions=translation_format_instructions)\n",
        "#\n",
        "#translation_chain = LLMChain(llm=translation_model, prompt=translation_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "7iWL3sX3YLN3",
        "outputId": "b86cb678-f8c7-4eb2-904e-c05fd5620dd3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'translation_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1b34336cd5bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m ).partial(format_instructions=translation_format_instructions)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtranslation_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslation_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'translation_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translation if needed chain:"
      ],
      "metadata": {
        "id": "KHFW06h1c2Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#def process_language_translation(df, text_column=\"sentence\"):\n",
        "#    df[\"detected_language\"] = df[text_column].apply(detect_language)\n",
        "#\n",
        "#    def translate_if_needed(row):\n",
        "#        if row[\"detected_language\"] == \"eng\":\n",
        "#            return row[text_column]\n",
        "#        else:\n",
        "#            response = translation_chain.invoke({\n",
        "#                \"source_language\": row[\"detected_language\"],\n",
        "#                \"sentence\": row[text_column]\n",
        "#            })\n",
        "#            return eval(response)[\"translated_text\"]\n",
        "#\n",
        "#    df[\"translated_text\"] = df.apply(translate_if_needed, axis=1)\n",
        "#    return df\n",
        "#\n",
        "#sample_df = m2_multilang_sentiment_df.sample(5, random_state=42)\n",
        "#processed_df = process_language_translation(sample_df)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "lfmtb2mgd4Sa",
        "outputId": "5afd9250-703c-4e4e-8dde-6912c2efafa5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'm2_multilang_sentiment_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-dd3926cb63b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm2_multilang_sentiment_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_language_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'm2_multilang_sentiment_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis Setup"
      ],
      "metadata": {
        "id": "Hm-KRPPUeqhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = \"\"\"\n",
        "Return only a valid JSON object with no additional text or formatting. The JSON object must contain exactly the following keys:\n",
        "{\n",
        "  \"sentiment\": \"positive\" | \"mixed\" | \"negative\",\n",
        "  \"explanation\": \"A brief reason for this classification in 25 words or less.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "sentiment_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an expert in sentiment analysis.\n",
        "    Analyze the following sentence and classify it as 'positive', 'negative', or 'mixed'.\n",
        "    Please think carefully about context and linguistic cues before making your classification.\n",
        "    If you are unsure, classify it as 'mixed'.\n",
        "    Then, explain your reasoning in 25 words or less.\n",
        "\n",
        "    Sentence: \"{sentence}\"\n",
        "\n",
        "    {format_instructions}\n",
        "    \"\"\"\n",
        ").partial(format_instructions=format_instructions)\n",
        "\n",
        "sentiment_chain = LLMChain(llm=granite_model, prompt=sentiment_prompt)"
      ],
      "metadata": {
        "id": "r8SPb4ahettT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(sentence):\n",
        "    response = sentiment_chain.invoke({\"sentence\": sentence})\n",
        "    parsed_response = json.loads(response[\"text\"])\n",
        "    return {\"sentiment\": parsed_response[\"sentiment\"], \"explanation\": parsed_response[\"explanation\"]}"
      ],
      "metadata": {
        "id": "3BTXzGKye65e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_chain.invoke(\"This is honestly great, and I have no complaints.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqmvH4V17CPl",
        "outputId": "9f4cd7b0-fb49-411c-ca32-c148954bab5a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': 'This is honestly great, and I have no complaints.',\n",
              " 'text': '\\nAssistant:\\n{\\n  \"sentiment\": \"positive\",\\n  \"explanation\": \"The sentence expresses strong satisfaction with the subject.\"\\n}'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toxicity Analysis Setup"
      ],
      "metadata": {
        "id": "DLTmm7qde_zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_format_instructions = \"\"\"\n",
        "Return a JSON object with only the following keys:\n",
        "- \"toxicity\": (toxic, non-toxic)\n",
        "- \"explanation\": (a brief explanation for why the toxicity was classified this way)\n",
        "Output only the JSON object. Do not add extra characters or whitespace.\n",
        "\"\"\"\n",
        "\n",
        "toxicity_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an expert in toxicity analysis, ie. detecting inappropriate and/or hurtful language.\n",
        "    Analyze the following sentence and classify it as 'toxic'or 'non-toxic'.\n",
        "    A sentence qualifies as 'toxic' if it contains rude, aggressive, offensive, or politically inappropriate language.\n",
        "    Then, explain your reasoning in 25 words or less.\n",
        "\n",
        "    Sentence: \"{sentence}\"\n",
        "\n",
        "    {toxic_format_instructions}\n",
        "    \"\"\"\n",
        ").partial(toxic_format_instructions=toxic_format_instructions)\n",
        "\n",
        "toxicity_chain = LLMChain(llm=granite_model, prompt=toxicity_prompt)"
      ],
      "metadata": {
        "id": "NP28pN19fHla"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_toxicity(sentence):\n",
        "    response = toxicity_chain.invoke({\"sentence\": sentence})\n",
        "\n",
        "    try:\n",
        "        parsed_response = json.loads(response[\"text\"])  # Attempt to parse JSON\n",
        "        return {\n",
        "            \"toxicity\": parsed_response.get(\"toxicity\", \"unknown\"),\n",
        "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided.\")\n",
        "        }\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON parsing failed for sentence: {sentence}\\nError: {e}\\nResponse: {response['text']}\")\n",
        "        return {\"toxicity\": \"unknown\", \"explanation\": \"Malformed response.\"}  # Safe fallback"
      ],
      "metadata": {
        "id": "sJp4w5JLfHZs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detoxification Setup"
      ],
      "metadata": {
        "id": "GsqAxsD8qSdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "detox_format_instructions = \"\"\"\n",
        "Return only a valid JSON object with the following keys:\n",
        "{\n",
        "  \"detoxified_text\": \"A rewritten version of the input sentence that removes toxicity while keeping the original intent.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "detox_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an AI assistant that removes toxicity from text.\n",
        "    Rewrite the following sentence to be non-toxic while preserving its original meaning.\n",
        "    Non-toxic means that the sentence should be free of offensive, harmful, or inappropriate language.\n",
        "\n",
        "    Toxic Sentence: \"{sentence}\"\n",
        "\n",
        "    {format_instructions}\n",
        "    \"\"\"\n",
        ").partial(format_instructions=detox_format_instructions)\n",
        "\n",
        "detox_chain = LLMChain(llm=granite_model, prompt=detox_prompt)"
      ],
      "metadata": {
        "id": "kLycpUneqarx"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#def detoxify_text(row):\n",
        "#    if row[\"prediction\"] == \"toxic\":\n",
        "#        response = detox_chain.invoke({\"sentence\": row[\"text\"]})\n",
        "#        raw_text = response[\"text\"].strip()\n",
        "#\n",
        "#        if raw_text.startswith(\"```json\") and raw_text.endswith(\"```\"):\n",
        "#            raw_text = raw_text[7:-3].strip()\n",
        "#\n",
        "#        raw_text = re.sub(r'[\\x00-\\x1F\\x7F]', '', raw_text)\n",
        "#\n",
        "#        try:\n",
        "#            parsed_response = json.loads(raw_text)\n",
        "#            return parsed_response.get(\"detoxified_text\", row[\"text\"])\n",
        "#        except json.JSONDecodeError as e:\n",
        "#            print(f\"JSON parsing failed for detox: {row['text']}\\nError: {e}\\nResponse: {raw_text}\")\n",
        "#            return row[\"text\"]\n",
        "#    else:\n",
        "#        return row[\"text\"]"
      ],
      "metadata": {
        "id": "nuA3du0vrnPr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agentive_workflow(text, downstream_task):\n",
        "    \"\"\"\n",
        "    Agentive workflow:\n",
        "    1. Detect the language of the text\n",
        "    2. If text not in English, translate to English\n",
        "    3. Routes the English text to the selected downstream task\n",
        "    downstream_task must be one of: 'sentiment', 'toxicity', or 'detoxification'.\n",
        "    \"\"\"\n",
        "    # 1. Language detection\n",
        "    detected_lang = detect_language(text)\n",
        "    if detected_lang != \"eng\":\n",
        "        # 2. Translation step if the language is not English\n",
        "        # Add 'eng: ' to start of string so that Toucan knows to translate to English\n",
        "        text = \"eng: \" + text\n",
        "        # Call\n",
        "        input_ids = toucan_tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(\"cuda:0\")\n",
        "        with torch.no_grad():\n",
        "          generated_ids = toucan_model.generate(**input_ids, num_beams=5, max_new_tokens=len(text), do_sample=True, temperature=0.6, top_p=0.9)\n",
        "        english_text = toucan_tokenizer.batch_decode(generated_ids, skip_special_tokens=True,  skip_prompt=True)[0]\n",
        "    else:\n",
        "        english_text = text\n",
        "\n",
        "    # 3. Downstream task execution - call the appropriate LLM Chain based on task\n",
        "    if downstream_task == \"sentiment\":\n",
        "        return analyze_sentiment(english_text)\n",
        "    elif downstream_task == \"toxicity\":\n",
        "        return analyze_toxicity(english_text)\n",
        "    # A few extra steps for detoxification\n",
        "    elif downstream_task == \"detoxification\":\n",
        "        # First, check if the text is toxic\n",
        "        toxicity_result = analyze_toxicity(english_text)\n",
        "        if toxicity_result[\"toxicity\"] == \"toxic\":\n",
        "            detox_response = detox_chain.invoke({\"sentence\": english_text})\n",
        "            raw_text = detox_response[\"text\"].strip()\n",
        "            # Deal with markdown and handle resulting errors gracefully\n",
        "            if raw_text.startswith(\"```json\") and raw_text.endswith(\"```\"):\n",
        "                raw_text = raw_text[7:-3].strip()\n",
        "            raw_text = re.sub(r'[\\x00-\\x1F\\x7F]', '', raw_text)\n",
        "            try:\n",
        "                parsed_response = json.loads(raw_text)\n",
        "                return {\"detoxified_text\": parsed_response.get(\"detoxified_text\", english_text)}\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON parsing failed during detoxification: {e}\\nResponse: {raw_text}\")\n",
        "                return {\"detoxified_text\": english_text}\n",
        "        else:\n",
        "            return {\"detoxified_text\": english_text}\n",
        "    else:\n",
        "        return {\"error\": \"Invalid downstream task. Please choose 'sentiment', 'toxicity', or 'detoxification'.\"}"
      ],
      "metadata": {
        "id": "bPV8jnvVxluw"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the data:"
      ],
      "metadata": {
        "id": "iw1JlRdegOTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test run of a single non-english, negative sentiment text\n",
        "\n",
        "agentive_workflow(\"Muundo wa tovuti hii ni maridadi, lakini ina mizigo ya matangazo kila ukurasa, inakera sana.\", \"sentiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNFVoHmX5VrQ",
        "outputId": "d2824e8e-c0f4-4c34-d543-f4b9b77045b6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'mixed',\n",
              " 'explanation': \"The sentence contains positive sentiment towards the website's structure, but negative sentiment about intrusive advertisements and potential dangers.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_test_path = \"/content/drive/MyDrive/MDS/COLX_565/data/Milstone-2-multilingual-sentiment-test-solutions.csv\"\n",
        "m2_multilang_sentiment_df = pd.read_csv(sentiment_test_path)\n",
        "\n",
        "sentiment_results = []\n",
        "for sentence in m2_multilang_sentiment_df[\"sentence\"]:\n",
        "    try:\n",
        "        response = agentive_workflow(sentence, \"sentiment\")\n",
        "        sentiment_results.append(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sentence: {sentence}\\nError: {e}\")\n",
        "        sentiment_results.append({\"sentiment\": \"unknown\", \"explanation\": \"Failed to process.\"})\n",
        "\n",
        "m2_multilang_sentiment_df[\"prediction\"] = [res[\"sentiment\"] for res in sentiment_results]\n",
        "m2_multilang_sentiment_df[\"explanation\"] = [res[\"explanation\"] for res in sentiment_results]\n",
        "\n",
        "true_sentiment = m2_multilang_sentiment_df[\"class-label\"]\n",
        "pred_sentiment = m2_multilang_sentiment_df[\"prediction\"]\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_sentiment, pred_sentiment, average=\"macro\")\n",
        "accuracy = accuracy_score(true_sentiment, pred_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iILlxUTJgaDc",
        "outputId": "1d15dc03-cf80-4639-c530-89da468a6baf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toxicity_test_path = \"/content/drive/MyDrive/MDS/COLX_565/data/Milestone-2-toxic-test-solutions.csv\"\n",
        "m2_toxicity_df = pd.read_csv(toxicity_test_path)\n",
        "\n",
        "\n",
        "toxicity_results = m2_toxicity_df[\"text\"].apply(analyze_toxicity)\n",
        "m2_toxicity_df[\"prediction\"] = toxicity_results.apply(lambda x: x[\"toxicity\"])\n",
        "m2_toxicity_df[\"explanation\"] = toxicity_results.apply(lambda x: x[\"explanation\"])\n",
        "\n",
        "# compute evaluation metrics\n",
        "true_toxicity = m2_toxicity_df[\"source_label\"]\n",
        "pred_toxicity = m2_toxicity_df[\"prediction\"]\n",
        "\n",
        "tox_precision, tox_recall, tox_f1, _ = precision_recall_fscore_support(true_toxicity, pred_toxicity, average=\"macro\")\n",
        "tox_accuracy = accuracy_score(true_toxicity, pred_toxicity)"
      ],
      "metadata": {
        "id": "J-xFe7rrggVr"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tox_precision, tox_recall, tox_f1, _ = precision_recall_fscore_support(true_toxicity, pred_toxicity, average=\"macro\")\n",
        "tox_accuracy = accuracy_score(true_toxicity, pred_toxicity)"
      ],
      "metadata": {
        "id": "RIR0yP64s6Yx"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m2_multilang_sentiment_df.to_csv(\"sentiment_analysis_results.csv\", index=False)\n",
        "m2_toxicity_df.to_csv(\"toxicity_analysis_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "-75jVpYpkfPV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentiment Analysis Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall: {recall:.3f}\")\n",
        "print(f\"F1 Score: {f1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-6y1y6_w5Sk",
        "outputId": "eac286f9-efe9-4a3f-cf94-9b4528a9bea6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Evaluation:\n",
            "Accuracy: 0.710\n",
            "Precision: 0.797\n",
            "Recall: 0.693\n",
            "F1 Score: 0.664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nToxicity Detection Evaluation:\")\n",
        "print(f\"Accuracy: {tox_accuracy:.3f} \")\n",
        "print(f\"Precision: {tox_precision:.3f}\")\n",
        "print(f\"Recall: {tox_recall:.3f}\")\n",
        "print(f\"F1 Score: {tox_f1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zITNR2I6iwyZ",
        "outputId": "beb688f8-b92e-4ad9-f0b2-7bcc86793d4d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Toxicity Detection Evaluation:\n",
            "Accuracy: 0.580 \n",
            "Precision: 0.465\n",
            "Recall: 0.492\n",
            "F1 Score: 0.407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detoxify:"
      ],
      "metadata": {
        "id": "n8Hayuekr3XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m2_toxicity_df[\"detoxified_text\"] = m2_toxicity_df[\"text\"].apply(lambda text: agentive_workflow(text, downstream_task=\"detoxification\"))['detoxified_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzcRUZRcr3A8",
        "outputId": "7de9ca5c-b964-4e6b-a202-14166e064656"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON parsing failed during detoxification: Expecting value: line 1 column 1 (char 0)\n",
            "Response: Response:{  \"detoxified_text\": \"Get lost, you Dutch person.\"}\n",
            "JSON parsing failed during detoxification: Expecting value: line 1 column 1 (char 0)\n",
            "Response: Response:{  \"detoxified_text\": \"2. In reality, he is a protected species, a Democrat, and a member of a racial minority group.\"}\n",
            "JSON parsing failed during detoxification: Expecting value: line 1 column 1 (char 0)\n",
            "Response: Assistant:{  \"detoxified_text\": \"2. #VoteNOonGillum #Gillum: Under scrutiny for alleged anti-American sentiments, Gillum is being investigated by the FBI. Two ongoing investigations and his association with certain political figures have raised questions. This situation bears a resemblance to past controversies, such as those involving former President Obama.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows of the DataFrame\n",
        "print(m2_toxicity_df[[\"text\", \"prediction\", \"detoxified_text\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YONEvnjBuieu",
        "outputId": "b854e7bd-96aa-4cc0-cb5b-253c7d25184a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text prediction  \\\n",
            "0  I know he's an idiot I think he might of been ...      toxic   \n",
            "1    ==my butthole==  i love buttholes. they tast...      toxic   \n",
            "2  This is absolute bullshit! We fucking knew thi...      toxic   \n",
            "3  Moochelle Nobama takes swipe at Trump it sez. ...      toxic   \n",
            "4  2. \\t> Lol, I guessed IPN (the Institue of Nat...      toxic   \n",
            "\n",
            "                                     detoxified_text  \n",
            "0  {'detoxified_text': 'I suspect he may have fac...  \n",
            "1  {'detoxified_text': 'I have a strong affinity ...  \n",
            "2  {'detoxified_text': 'It's incredibly disappoin...  \n",
            "3  {'detoxified_text': 'Michelle Obama reportedly...  \n",
            "4  {'detoxified_text': '2. > I anticipated IPN (t...  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4827227cb4274a61829bb6ba13a199ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dce9d00434f428d83bf5515a7478816",
              "IPY_MODEL_6e70f9aad9b24139ad192297d65b4c90",
              "IPY_MODEL_11cf8e490fdb407e87459e6370f3ab2d"
            ],
            "layout": "IPY_MODEL_30a642692b9d4aa0b423cc558d1074b5"
          }
        },
        "5dce9d00434f428d83bf5515a7478816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dafcab5097342828681c06845ca2a75",
            "placeholder": "​",
            "style": "IPY_MODEL_2757d53c0ed2484696b9f6b35565903d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6e70f9aad9b24139ad192297d65b4c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ce6256c1a61406cb96bd12ab16ef8a5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b269f07013104e3483f9dc970d5c7378",
            "value": 2
          }
        },
        "11cf8e490fdb407e87459e6370f3ab2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d05321c1f2a34b789cc89711da65023f",
            "placeholder": "​",
            "style": "IPY_MODEL_9294415bf5ab4595946ae233981b35ef",
            "value": " 2/2 [00:02&lt;00:00,  1.18s/it]"
          }
        },
        "30a642692b9d4aa0b423cc558d1074b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dafcab5097342828681c06845ca2a75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2757d53c0ed2484696b9f6b35565903d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ce6256c1a61406cb96bd12ab16ef8a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b269f07013104e3483f9dc970d5c7378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d05321c1f2a34b789cc89711da65023f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9294415bf5ab4595946ae233981b35ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}